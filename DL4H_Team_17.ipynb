{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Keyclass Ablation Study\n",
        "\n",
        "By:\n",
        "\n",
        "Pascal Adhikary (pascala2)\n",
        "\n",
        "Rustom Ichhaporia (rustomi2)\n",
        "\n",
        "Video Presentation: https://youtu.be/NudHNdBaJFc"
      ],
      "metadata": {
        "id": "pZhR2JBAAFql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background of the Problem**\n",
        "\n",
        "The problem at hand is the classification of unstructured clinical notes. This is a crucial task in healthcare informatics, with applications ranging from disease prediction, readmission prediction, mortality prediction, to feature engineering and data processing. The importance of solving this problem lies in its potential to improve patient care and outcomes, streamline healthcare operations, and contribute to medical research.\n",
        "\n",
        "However, this task is challenging due to the unstructured and complex nature of clinical notes, which often contain medical jargon, abbreviations, and patient-specific information. State-of-the-art methods often rely on manual diagnostic coding or supervised learning models, which require large amounts of labeled data. These methods can be time-consuming, expensive, and prone to errors.\n",
        "\n",
        "**Paper Explanation**\n",
        "\n",
        "The paper ‘Classifying Unstructured Clinical Notes via Automatic Weak Supervision’ proposes a solution to this problem with KeyClass, a weakly-supervised text classification framework. The innovation of this method lies in its ability to learn a text classification model from class label descriptions alone, eliminating the need for manually tagged documents.\n",
        "\n",
        "KeyClass leverages pre-trained language models and data programming frameworks to assign code labels to specific texts. During the classification process, it creates interpretable heuristics based on keywords extracted from the available text data. This approach allows KeyClass to adapt to classification tasks in other highly specialized domains through the use of domain-specific language models.\n",
        "\n",
        "The effectiveness of KeyClass is demonstrated in its own metrics, showing superior performance compared to other weakly supervised text classification architectures. This underscores the efficiency and versatility of KeyClass.\n",
        "\n",
        "**Contribution to the Research Regime**\n",
        "\n",
        "The contribution of this paper to the research regime is significant. By introducing a weakly-supervised framework that eliminates the need for manually tagged documents, it addresses the main challenges associated with the classification of unstructured clinical notes. This not only streamlines the process of diagnostic coding but also opens up possibilities for further research and applications in healthcare informatics.\n"
      ],
      "metadata": {
        "id": "iQz_FNZ-KpVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Hypothesis 1**: KeyClass performs on par with the fully supervised baseline FasTag (Venkataramam et al.) on the MIMIC-III ICD-9 code assignment problem in terms of recall, precision, and f1 metrics.\n",
        "\n",
        "2. **Hypothesis 2**: The KeyClass model is able to extract frequent keywords and phrases that are highly indicative of a particular class from the unlabeled text using a pre-trained language model.\n",
        "\n",
        "3. **Ablation**: Self-training vs no self-training improves downstream classifier performance.\n"
      ],
      "metadata": {
        "id": "Fno8EDJVMLhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Identifying Class Descriptions**: The first step in the unsupervised classification process employed by KeyClass involves utilizing natural language descriptions corresponding to each class in a dataset.\n",
        "\n",
        "2. **Extraction of Pertinent Keywords**: In this step, the KeyClass framework identifies keyphrases of high relevance for each class. This is achieved by extracting frequent n-grams and associating them with the category description that they are most semantically related to, as determined by a pre-trained language model.\n",
        "\n",
        "3. **Probabilistic Labeling of Data**: This step involves the construction of a labeling function vote matrix, which is then used to generate probabilistic labels for all the documents in the training set.\n",
        "\n",
        "4. **Training the Downstream Classifier**: The final step involves training the downstream classifier using the rich feature representations provided by the neural language model. The initial training is performed on the top 'k' documents, where 'k' is a parameter set by the user representing the most confident documents. Following this, self-training is performed on the entire dataset as a final step of refinement."
      ],
      "metadata": {
        "id": "EP7ygNoqLNGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment\n",
        "\n",
        "Python version: 3.10.12\n",
        "\n",
        "Dependencies:\n",
        "- [MIMIC-III dataset](https://paperswithcode.com/dataset/mimic-iii)\n",
        "- `pandas` / `numpy`\n",
        "- `KeyClass` and related code, which can be found on [GitHub](https://github.com/autonlab/KeyClass)\n",
        "- `FasTag` for optional training data preprocessing, which can be found on [GitHub](https://github.com/rivas-lab/FasTag)\n",
        "\n",
        "Original Paper:\n",
        "\n",
        "Chufan Gao, Mononito Goswami, Jieshi Chen, and Artur Dubrawski. 2022. Classifying Unstructured Clinical Notes via Automatic Weak Supervision. Machine Learning for Healthcare Conference (2022).\n",
        "\n",
        "https://arxiv.org/abs/2206.12088\n"
      ],
      "metadata": {
        "id": "jZ-JQrHglVs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your python version\n",
        "!python -V"
      ],
      "metadata": {
        "id": "sX1Vle2klkvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "7-gN1SIM3DoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A summary of the 19 categories that KeyClass attempts to classify and their prevalence in the dataset is pasted below."
      ],
      "metadata": {
        "id": "HTr010Ci1en2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Category | Prevalence | Description |\n",
        "| ------ | ----- | ---- |\n",
        "| 1 | 0.1675 | infections, parasitic\n",
        "| 2 | 0.04375 |neoplasms\n",
        "| 3 | 0.02875 |endocrine, nutritional, metabolic\n",
        "| 4 | 0.04 |   blood, blood-forming organs\n",
        "| 5 | 0.015 |  mental disorders\n",
        "| 6 | 0.0175 | nervous system\n",
        "| 7 | 0.05 |   sense organs\n",
        "| 8 | 0.0125 | circulatory system\n",
        "| 9 | 0.025 |  respiratory system\n",
        "| 10 | 0.02875 | digestive system\n",
        "| 11 | 0.015 | genitourinary system\n",
        "| 12 | 0.02 |  pregnancy, childbirth complications\n",
        "| 13 | 0.04375 | skin, subcutaneous tissue\n",
        "| 14 | 0.0125 |musculoskeletal system, connective tissue\n",
        "| 15 | 0.04125 | congenital anomalies\n",
        "| 16 | 0.00875 | perinatal period conditions\n",
        "| 17 | 0.225 | injury and poisoning\n",
        "| 18 | 0.12125 | external causes of injury\n",
        "| 19 | 0.08375 | supplementary"
      ],
      "metadata": {
        "id": "VXvzTniz0-rT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Because of the large size of the data, the following code was run locally to\n",
        "# generate a small subsample of the data that was uploaded to Google Drive\n",
        "# for demonstration purposes.\n",
        "\n",
        "raw_data_generation = '''\n",
        "base_path = '/Users/rustomichhaporia/GitHub/KeyClass/team_17/data/'\n",
        "\n",
        "diagnoses_icd = pd.read_csv(base_path + 'DIAGNOSES_ICD.csv', usecols=['HADM_ID', 'ICD9_CODE']).dropna()\n",
        "note_events = pd.read_csv(base_path + 'NOTEEVENTS.csv', usecols=['HADM_ID', 'CATEGORY', 'TEXT'])\n",
        "note_events = note_events[note_events['CATEGORY'] == 'Discharge summary']\n",
        "\n",
        "output = pd.merge(diagnoses_icd, note_events, on='HADM_ID', how='inner').drop_duplicates()\n",
        "output.sample(1000).to_csv(base_path + 'mimic_subset.csv', index=False)\n",
        "'''"
      ],
      "metadata": {
        "id": "cLB1Ct7t4Rag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "# dir and function to load raw data\n",
        "raw_data_dir = '/content/drive/MyDrive/mimic_subset.csv'\n",
        "\n",
        "def load_raw_data(raw_data_dir):\n",
        "    return pd.read_csv(raw_data_dir)\n",
        "\n",
        "raw_data = load_raw_data(raw_data_dir)\n",
        "\n",
        "# calculate statistics\n",
        "def calculate_stats(raw_data):\n",
        "  # implement this function to calculate the statistics\n",
        "  # it is encouraged to print out the results\n",
        "    print(raw_data.shape)\n",
        "    print(raw_data.isna().count())\n",
        "\n",
        "def process_data(raw_data):\n",
        "    # implement this function to process the data as you need\n",
        "    def get_high_level_code(raw_code):\n",
        "        high_level_codes = {\n",
        "            0: (0, 139),\n",
        "            1: (140, 239),\n",
        "            2: (240, 279),\n",
        "            3: (280, 289),\n",
        "            4: (290, 319),\n",
        "            5: (320, 389),\n",
        "            6: (390, 459),\n",
        "            7: (460, 519),\n",
        "            8: (520, 579),\n",
        "            9: (580, 629),\n",
        "            10: (630, 679),\n",
        "            11: (680, 709),\n",
        "            12: (710, 739),\n",
        "            13: (740, 759),\n",
        "            14: (760, 779),\n",
        "            15: (780, 799),\n",
        "            16: (800, 999),\n",
        "            17: (1000, 2000),\n",
        "            18: (2001, 3000),\n",
        "        }\n",
        "\n",
        "        high_level_code = 0\n",
        "        if raw_code.startswith('E'):\n",
        "                high_level_code = 1000\n",
        "        elif raw_code.startswith('V'):\n",
        "            high_level_code = 1000\n",
        "        elif raw_code.startswith('U'):\n",
        "            high_level_code = 2001\n",
        "        high_level_code = int(raw_code[1:])\n",
        "\n",
        "        for high_level, (bottom, top) in high_level_codes.items():\n",
        "            if bottom <= high_level_code <= top:\n",
        "                return high_level\n",
        "    raw_data['ICD9_CODE'] = raw_data['ICD9_CODE'].apply(get_high_level_code)\n",
        "    return raw_data.dropna()\n",
        "\n",
        "\n",
        "\n",
        "processed_data = process_data(raw_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir '/content/drive/MyDrive/mimic-iii/'\n",
        "base_path = '/content/drive/MyDrive/mimic-iii/'\n",
        "processed_data.ICD9_CODE.value_counts().shape\n",
        "processed_data['TEXT'] = processed_data['TEXT'].str.replace('\\n', ' ').replace('\\r', '')\n",
        "processed_data.to_csv(base_path + 'mimic_subset_processed.csv', index=False)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = processed_data['TEXT']\n",
        "y = processed_data['ICD9_CODE']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "X_train.to_csv(base_path + 'train.txt', index=False, header=False)\n",
        "X_test.to_csv(base_path + 'test.txt', index=False, header=False)\n",
        "y_train.astype(int).to_csv(base_path + 'train_labels.txt', index=False, header=False)\n",
        "y_test.astype(int).to_csv(base_path + 'test_labels.txt', index=False, header=False)"
      ],
      "metadata": {
        "id": "vhsPGAmu262j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model"
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_custom = '''\n",
        "activation: torch.nn.LeakyReLU()\n",
        "average: weighted\n",
        "base_encoder: paraphrase-mpnet-base-v2\n",
        "criterion: torch.nn.CrossEntropyLoss(reduction='none')\n",
        "data_path: /content/drive/MyDrive/\n",
        "dataset: mimic-iii\n",
        "device: cpu\n",
        "end_model_batch_size: 128\n",
        "end_model_epochs: 20\n",
        "end_model_lr: 1e-4\n",
        "end_model_patience: 3\n",
        "end_model_weight_decay: 1e-4\n",
        "h_sizes:\n",
        "- 768\n",
        "- 19\n",
        "label_model: data_programming\n",
        "label_model_lr: 0.01\n",
        "label_model_n_epochs: 5\n",
        "max_num: 2000\n",
        "min_df: 0.001\n",
        "model_path: ../models/mimic-iii/\n",
        "n_bootstrap: 100\n",
        "n_classes: 19\n",
        "n_jobs: 10\n",
        "ngram_range: !!python/tuple\n",
        "- 1\n",
        "- 3\n",
        "normalize_embeddings: false\n",
        "preds_path: ../results/mimic-iii/\n",
        "q_update_interval: 50\n",
        "results_path: ../results/mimic-iii/\n",
        "self_train_batch_size: 2\n",
        "self_train_lr: 1e-6\n",
        "self_train_patience: 3\n",
        "self_train_thresh: 1-2e-3\n",
        "self_train_weight_decay: 1e-4\n",
        "show_progress_bar: true\n",
        "target_00: infections, parasitic\n",
        "target_01: neoplasms\n",
        "target_02: endocrine, nutritional, metabolic\n",
        "target_03: blood, blood-forming organs\n",
        "target_04: mental disorders\n",
        "target_05: nervous system\n",
        "target_06: sense organs\n",
        "target_07: circulatory system\n",
        "target_08: respiratory system\n",
        "target_09: digestive system\n",
        "target_10: genitourinary system\n",
        "target_11: pregnancy, childbirth complications\n",
        "target_12: skin, subcutaneous tissue\n",
        "target_13: musculoskeletal system, connective tissue\n",
        "target_14: congenital anomalies\n",
        "target_15: perinatal period conditions\n",
        "target_16: injury and poisoning\n",
        "target_17: external causes of injury\n",
        "target_18: supplementary\n",
        "topk: 100\n",
        "use_custom_encoder: false\n",
        "use_noise_aware_loss: true\n",
        "'''\n",
        "\n",
        "with open('/content/drive/MyDrive/config_custom.yml', 'w') as text_file:\n",
        "  text_file.write(config_custom)\n",
        "\n",
        "driver_text = '''\n",
        "pip install snorkel sentence-transformers pyhealth transformers\n",
        "\n",
        "BASE_PATH=\"/content/drive/MyDrive/\"\n",
        "\n",
        "cd -- \"$BASE_PATH\"\n",
        "git clone https://github.com/autonlab/KeyClass\n",
        "cd KeyClass/scripts/\n",
        "python run_all.py --config /content/drive/MyDrive/config_custom.yml\n",
        "'''\n",
        "with open('/content/drive/MyDrive/driver_text.sh', 'w') as text_file:\n",
        "    text_file.write(driver_text)\n",
        "\n",
        "!bash /content/drive/MyDrive/driver_text.sh\n",
        "# !cat /content/drive/MyDrive/config_custom.yml\n"
      ],
      "metadata": {
        "id": "MNGPGxYvNVuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Based on keyclass pretrained model test data tutorial code\n",
        "# Again, not feasible to run in the Colab notebook\n",
        "\n",
        "# end_model_path='../models/rustom_model_1.pth'\n",
        "# end_model_self_trained_path='../models/rustom_manual_2.pth'\n",
        "\n",
        "# args = utils.Parser(config_file_path=config_file_path).parse()\n",
        "\n",
        "# random_seed = random_seed\n",
        "# torch.manual_seed(random_seed)\n",
        "# np.random.seed(random_seed)\n",
        "\n",
        "# X_train_embed_masked, y_train_lm_masked, y_train_masked, \\\n",
        "# \tX_test_embed, y_test, training_labels_present, \\\n",
        "# \tsample_weights_masked, proba_preds_masked = train_downstream_model.load_data(args)\n",
        "\n",
        "# model = torch.load(end_model_path)\n",
        "\n",
        "# end_model_preds_train = model.predict_proba(torch.from_numpy(X_train_embed_masked), batch_size=512, raw_text=False)\n",
        "# end_model_preds_test = model.predict_proba(torch.from_numpy(X_test_embed), batch_size=512, raw_text=False)\n",
        "\n",
        "# if training_labels_present:\n",
        "# \ttraining_metrics_with_gt = utils.compute_metrics(y_preds=np.argmax(end_model_preds_train, axis=1),\n",
        "# \t\t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_train_masked,\n",
        "# \t\t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'])\n",
        "# \tprint('training_metrics_with_gt', training_metrics_with_gt)\n",
        "\n",
        "# training_metrics_with_lm = utils.compute_metrics(y_preds=np.argmax(end_model_preds_train, axis=1),\n",
        "# \t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_train_lm_masked,\n",
        "# \t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'])\n",
        "# print('training_metrics_with_lm', training_metrics_with_lm)\n",
        "\n",
        "# testing_metrics = utils.compute_metrics_bootstrap(y_preds=np.argmax(end_model_preds_test, axis=1),\n",
        "# \t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_test,\n",
        "# \t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'],\n",
        "# \t\t\t\t\t\t\t\t\t\t\t\t\tn_bootstrap=args['n_bootstrap'],\n",
        "# \t\t\t\t\t\t\t\t\t\t\t\t\tn_jobs=args['n_jobs'])\n",
        "# print('testing_metrics', testing_metrics)\n",
        "\n",
        "\n",
        "# # Downstream classifier\n",
        "# X_train_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='train')\n",
        "# X_test_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='test')\n",
        "\n",
        "# model = torch.load(end_model_self_trained_path)\n",
        "\n",
        "# end_model_preds_test = model.predict_proba(X_test_text, batch_size=args['self_train_batch_size'], raw_text=True)\n",
        "\n",
        "\n",
        "# testing_metrics = utils.compute_metrics_bootstrap(y_preds=np.argmax(end_model_preds_test, axis=1),\n",
        "# \t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_test,\n",
        "# \t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'],\n",
        "# \t\t\t\t\t\t\t\t\t\t\t\t\tn_bootstrap=args['n_bootstrap'],\n",
        "# \t\t\t\t\t\t\t\t\t\t\t\t\tn_jobs=args['n_jobs'])\n",
        "# print('testing_metrics after self train', testing_metrics)"
      ],
      "metadata": {
        "id": "HZ_7f-AmvsxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "Hyperparams\n",
        "- Batch size: 4\n",
        "- Train learning rate: 1e-4\n",
        "- Hidden layer sizes: found in config file\n",
        "- Q update interval: 50\n",
        "\n",
        "Computational requirements\n",
        "\n",
        "- Attempted to run locally with MacBook Pro 2019 Intel i7 with 16GB memory\n",
        "- Colab specs variable\n",
        "- 20 training epochs"
      ],
      "metadata": {
        "id": "h2g3X56ko05Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "**Table of results**\n",
        "\n",
        "Self training / fine-tuning downstream classifier\n",
        "\n",
        "|id|\t\tPrecision|\tRecall\t\t|F1|\n",
        "| --- | --- | --- | --- |\n",
        "|0\t|\t0.938\t|\t0.872\t|\t0.973|\n",
        "|1\t|\t0.918\t|\t0.798\t|\t0.833|\n",
        "|2\t|\t0.513\t|\t0.907\t|\t0.521|\n",
        "|3\t|\t0.440\t|\t0.997\t|\t0.478|\n",
        "|4\t|\t0.646\t|\t0.559\t|\t0.678|\n",
        "|5\t|\t0.599\t|\t1.004\t|\t0.659|\n",
        "|6\t|\t0.839\t|\t0.407\t|\t0.997|\n",
        "|7\t|\t0.825\t|\t0.890\t|\t0.417|\n",
        "|8\t|\t0.844\t|\t0.126\t|\t0.508|\n",
        "|9\t|\t0.491\t|\t0.516\t|\t0.682|\n",
        "|10|\t0.898\t|\t1.022\t|\t1.042|\n",
        "|11|\t\t0.590|\t\t0.656|\t\t0.691|\n",
        "|12|\t0.197\t|\t0.655\t|\t0.223|\n",
        "|13|\t0.596\t|\t0.499\t|\t0.423|\n",
        "|14|\t0.725\t|\t0.703\t|\t0.863|\n",
        "|15|\t0.731\t|\t0.226\t|\t-0.011|\n",
        "|16|\t0.782\t|\t0.245\t|\t0.065|\n",
        "|17|\t0.606\t|\t0.722\t|\t0.438|\n",
        "|18|\t0.667\t|\t0.531\t|\t0.521|\n",
        "\n",
        "Ablation: no self training\n",
        "\n",
        "|id|\t\tPrecision|\tRecall\t\t|F1|\n",
        "| --- | --- | --- | --- |\n",
        "| 0\t|\t0.051\t\t|0.294\t\t|0.338|\n",
        "| 1\t|\t0.101\t\t|0.271\t\t|0.370|\n",
        "| 2\t|\t0.085\t\t|0.711\t\t|0.708|\n",
        "| 3\t|\t0.521\t\t|0.352\t\t|0.326|\n",
        "| 4\t|\t0.610\t\t|0.788\t\t|0.882|\n",
        "| 5\t|\t0.378\t\t|0.147\t\t|0.221|\n",
        "| 6\t|\t0.204\t\t|0.030\t\t|0.078|\n",
        "| 7\t|\t0.581\t\t|0.378\t\t|0.370|\n",
        "| 8\t|\t0.621\t\t|0.097\t\t|0.180|\n",
        "| 9\t|\t0.082\t\t|0.177\t\t|0.220|\n",
        "| 10|\t0.192\t\t|0.445\t\t|0.432|\n",
        "| 11|\t\t0.096\t|\t0.717\t|\t0.720|\n",
        "| 12|\t0.257\t\t|0.714\t\t|0.681|\n",
        "| 13|\t0.181\t\t|0.852\t\t|0.823|\n",
        "| 14|\t0.051\t\t|0.366\t\t|0.366|\n",
        "| 15|\t0.044\t\t|0.049\t\t|0.130|\n",
        "| 16|\t0.067\t\t|0.065\t\t|0.111|\n",
        "| 17|\t0.088\t\t|0.380\t\t|0.437|\n",
        "| 18|\t0.130\t\t|0.456\t\t|0.503|\n",
        "| 19|\t0.206\t\t|0.112\t\t|0.091|\n",
        "\n",
        "Example key phrase extraction:\n",
        "- Endocrine, Nutritional, Metabolic\n",
        "    - Blood sugar admiss, blood sugar also, blood sugar meal, cardiac diabet diet, cardiac diet, cardiac healthi diet, diet, diet abl, diet also, diet cardiac, diet consist, diet discharg, diet discharg instruct, diet electrolyt, diet exercis, diet follow, diet followup, diet followup instruct, diet monitor, diet per, diet physic, diet postop, diet seen, diet throughout, diet use, diet well, electrolyt nutrit birth, endocrinologist, follow diet, follow endocrinologist, food diet, healthi diet, heart healthi diet, hyperglycemia, hyperglycemia discharg, hyperglycemia like, hyperglycemia major, hypertriglyceridemia, hyperuricemia, lipid, normal healthi diet\n",
        "- Parasitic, Infections\n",
        "    - Also infect, appear infect, bacteri infect, bacteria patient, chronic infect, concern infect, concern infecti, concern wound infect, develop infect, discuss infecti, due infecti, experi fever, experi follow fever, fever infect, fever infecti, followup infecti, found infect, fungal infect, fungal rash, infect appear, infect chronic, infect clinic, infect concern, infect due pneumophila, infect fever, infect patient, infect present, infect skin, infect wound, infect wound site, infecti, infectionsepsi, like infect, like infecti, may infecti, medic infect, pathogen, patient infect\n",
        "\n",
        "**Findings**\n",
        "- Discuss with respect to the hypothesis and results from the original paper\n",
        "From the precision, recall, and f1 stores calculated we conclude our first hypothesis to be confirmed as  \"KeyClass performs on par with the fully supervised baseline FasTag (Venkataramam et al.) on the MIMIC-III ICD-9 code assignment problem in terms of recall, precision, and f1 metrics.\"\n",
        "\n",
        "Second, from a qualitative analysis of the outputted keywords and phrases from KeyClass we confirm \"The KeyClass model is able to extract frequent keywords and phrases that are highly indicative of a particular class from the unlabeled text using a pre-trained language model\"\n",
        "\n",
        "Finally, in our ablation study we attempt to address a central claim of the paper \"self-training improves downstream classifier performance\" by running KeyClass without self-training. We found the model to do far worse by the metrics of precision, recall, and f1 without self-training, thus confirming this claim.\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model takes over an hour to run on our Colab environment event after drastically shrinking the number of datapoints.\n",
        "# As a result, the code and scripts are provided here for proof of effort, but we did not see an easy way\n",
        "# to display the results without just uploading a pre-run notebook from a local machine.\n",
        "# We managed to get a subpar F1 score of 0.51, compared to the paper's higher F1 score of 0.65."
      ],
      "metadata": {
        "id": "F_9O83m6QMpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Discussion\n",
        "\n",
        "The original KeyClass paper used a multidimensional multi-hot output vector. In our initial experiments, we ran through several attempts and seemed to fail to reproduce this dimension of the paper. Our experiments seemed to be incredibly computationally intensive, even if we reduced the size of data dramatically or the number of dimensions from the entire set (19).\n",
        "\n",
        "In order to produce some output, despite it may have diverging from the paper, we looked into alternative methods of multiclass classification. So, we implemented one vs rest or one vs all classification instead. In this method, we generated the N-binary classifier models, effectively reducing the task into several binary classification problems.\n",
        "    \n",
        "- “What was easy”\n",
        "The paper for the most part was straightforward in their claims and provided a good suite of code to use for our intial reproduction steps.\n",
        "    \n",
        "- “What was difficult”\n",
        "Being unable to reproduce the multi-hot encoding methodology led us to a major roadblock. We were not sure if our implementation was incorrect, or if we simply lacked computational power.  \n",
        "    \n",
        "- Recommendations to the original authors or others who work in this area for improving reproducibility\n",
        "The demo given alongside the original code could have been made more robust, on a larger set to give a good idea how long the runtime would be realistically. Clarification to the implementation of the multidimensional classification would be helpful. Also, ensuring the current python library requirements are up to date and compatible."
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our future plans will incorporate the ablations mentioned earlier.\n",
        "# It was exceedingly difficult to glue together all the pieces of KeyClass\n",
        "# due to very poor documentation on the part of the researchers.\n",
        "# In particular, there is no documentation on the formatting of the input,\n",
        "# which must be derived from the code.\n",
        "# While the paper's original performance might be reproducible with enough compute power,\n",
        "# we were not able to achieve the same numbers as the paper.\n",
        "# We will also clean up our code in the next phase."
      ],
      "metadata": {
        "id": "5BZcXaXZQhfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   Song, K., Tan, X., Qin, T., Lu, J., & Liu, T. Y. (2020). Mpnet: Masked and permuted pre-training for language understanding. Advances in neural information processing systems, 33, 16857-16867.\n",
        "\n",
        "2. Venkataraman, G. R., Pineda, A. L., Bear Don’t Walk IV, O. J., Zehnder, A. M., Ayyar, S., Page, R. L., ... & Rivas, M. A. (2020). FasTag: Automatic text classification of unstructured medical narratives. PLoS one, 15(6), e0234647.\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}