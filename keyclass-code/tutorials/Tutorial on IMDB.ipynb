{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 📖 KeyClass Tutorial: Text Classification with Label-Descriptions Only\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "***Author(s):*** Arnab Dey, Chufan Gao, Mononito Goswami, correspondence to &lt;mgoswami@andrew.cmu.edu&gt;\n",
    "\n",
    "<img align=\"right\" src=\"../assets/autonlab_logo.png\" width=\"20%\"/>\n",
    "\n",
    "## Contents\n",
    "\n",
    "\n",
    "### 1. [Problem Background & Motivation](#introduction) \n",
    "\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  1.1 [Electronic Health Records (EHR)](#ehr)\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  1.2 [Generalizable Insights in Healthcare Contexts](#insights)\n",
    "\n",
    "\n",
    "### 2. [Methodology](#methodology) \n",
    "\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.1 [Prior Work](#prior)\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.2 [KeyClass](#keyclass)\n",
    "<!-- ####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.3 [Problem Formulation](#math) -->\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.3 [Find Class Descriptions](#classdesc)\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.4 [Find Relevant Keywords](#keywords)\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.5 [Probabilistically Labeling the Data](#label)\n",
    "\n",
    "\n",
    "### 3. [Experimentation: Training](#exp_training) \n",
    "\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  3.1 [Training the Downstream Model](#downstream)\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  3.2 [Self-Training the Model](#self)\n",
    "\n",
    "### 4. [Experimentation: Testing](#exp_testing) \n",
    "\n",
    "### 5. [References](#references) \n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## 1. Problem Background & Motivation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ehr'></a>\n",
    "### 1.1 Electronic Health Records (EHR) \n",
    "\n",
    "The Electronic Health Record __(EHR)__ system is a digital version of a patient’s paper chart. __EHRs__ are almost-real-time, patient-centered records that contain `patient history`, `diagnoses`, `procedures`, `medications`, and more in an easily accessible format. Since the _Health Information Technology for Economic and Clinical Health_ <b>(HITECH)</b> Act was signed into law in 2009, adoption rates of these systems have steadily increased<sup><a href=\"#references\"><b>1</b></a></sup>. Adler-Milstein et al.<sup><a href=\"#references\"><b>2</b></a></sup>, who analyzed survey data collected by American Hospital Association found that EHR adoption rates were at <code><b>80%</b></code> in __2017__, twice the rate in __2008__. With higher adoption rates comes the rising challenge of _data processing and analysis of unstructured clinical text._ Due to the unstructured nature of clinical notes, providers often employ trained staff and/or third-party vendors to help assign diagnostic codes using coding systems such as the International Classification of Diseases __(ICD)__<sup><a href=\"#references\"><b>3</b></a></sup>. \n",
    "\n",
    "__However, manual assignment of ICD codes is problematic:__\n",
    "1. It is both time consuming and error-prone, with only <code><b>60-80%</b></code> of the assigned codes reflecting actual patient diagnoses<sup><a href=\"#references\"><b>4</b></a></sup>\n",
    "1. A significant portion of code assignment results in misjudged severity of conditions and code omissions<sup><a href=\"#references\"><b>5</b></a></sup>\n",
    "1. For healthcare providers, billing and coding errors may not only lead to loss of revenue and claim denials, but also federal penalties for erroneous Medicare and Medicaid claims\n",
    "\n",
    "Thus, there is a clear need for reliable automated classification of unstructured clinical notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='insights'></a>\n",
    "### 1.2 Generalizable Insights in Healthcare Contexts \n",
    "\n",
    "Managing costs and quality of healthcare is a persistent societal challenge of enormous magnitude and impact on daily lives of all people. Our approach proposes a low-cost solution that has the potential to address some of the identified pressing issues with accessibility to affordable yet accurate automated disease coding tools. Our contributions lie in using a novel strategy\n",
    "to efficiently acquire interpretable weak supervision sources from readily available text to learn effective text classifiers without the need for human-labeled data.\n",
    "\n",
    "__Our work demonstrates:__\n",
    "1. Pre-trained language models can efficiently and effectively inform weakly supervised models for text classification\n",
    "1. Self-training improves downstream classifier performance, especially when classifiers are initially trained on a subset of the training data\n",
    "1. Data programming performs on par with simple majority vote when relying on a large number of automatically generated weak supervision sources of similar quality\n",
    "1. Keywords are excellent sources of weak supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='methodology'></a>\n",
    "## 2. Methodology \n",
    "\n",
    "<!-- <center> -->\n",
    "<img align=\"top\" src=\"../assets/KeyClass.png\" width=\"50%\"/>\n",
    "<!-- </center> -->\n",
    "\n",
    "<b>Figure A:</b> Overview of our methodology. From only class descriptions, KeyClass classifies documents without access to any labeled data. It automatically creates interpretable labeling functions (LFs) by extracting frequent keywords and phrases that are highly indicative of a particular class from the unlabeled text using a pre-trained language model. It then uses these LFs along with Data Programming (DP) to generate probabilistic labels for training data, which are used to train a downstream classifier <sup><a href=\"#references\"><b>13</b></a></sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prior'></a>\n",
    "### 2.1 Prior Work\n",
    "\n",
    "__Assigning ICD codes to Clinical Notes<sup><a href=\"#references\"><b>[6,7,8]</b></a></sup>:__\n",
    "1. To the best of our knowledge, all prior work on ICD code assignment utilized __fully supervised ML techniques__, most of them relying on vast quantities of labeled training data\n",
    "1. In this work, we explore the use of our proposed __weakly supervised model _KeyClass___ to assign top-level `ICD-9` codes to long patient discharge summaries\n",
    "1. Its training signal is retrieved automatically from readily available descriptions of the ICD codes, therefore it requires no human-produced supervisory feedback to build effective downstream text classifiers\n",
    "\n",
    "__Text Classification with Sparse Training Labels<sup><a href=\"#references\"><b>[9,10]</b></a></sup>:__\n",
    "1. Our work differs from prior work because the foundation of our weak supervision methodology, i.e., frequent keywords and phrases as LFs, is highly interpretable\n",
    "1. Secondly, while previously proposed state-of-the-art models are committed to specific language model architectures for linguistic knowledge and representation learning, KeyClass offers a high degree of modularity, enabling end users to adapt the neural language model (encoder) and downstream classifiers to specific problems, such as clinical text classification\n",
    "1. Finally, we explore the use of weak supervision for multilabel multiclass classification, a problem which, to the best of our knowledge, has not been tackled by prior work on weak text classification\n",
    "\n",
    "__Weak Supervision for Clinical Text Classification<sup><a href=\"#references\"><b>[11,12]</b></a></sup>:__\n",
    "1. Prior work on weakly supervised clinical text classification had an explicit dependence on manually created rule-based labeling functions\n",
    "1. In this work, however, we demonstrate that we can quickly and automatically create simple keyword based labeling functions, with minimal to no human involvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='keyclass'></a>\n",
    "### 2.2 KeyClass\n",
    "\n",
    "As a potential remedy, we present KeyClass, a general weakly supervised text classification framework combining Data Programming<sup><a href=\"#references\"><b>13</b></a></sup> with a novel method of automatically acquiring interpretable weak supervision sources (keywords and phrases) from class-label descriptions only without the need to access to any labeled documents. The successful application of KeyClass to solve an important clinical text classification problem demonstrates its potential for making social impact by allowing quick and affordable development and deployment of effective text classifiers.\n",
    "\n",
    "<img align=\"top\" src=\"../assets/flowchart.png\" width=\"50%\"/>\n",
    "\n",
    "<b>Figure B:</b> Data programming, or weak supervision compared to fully supervised ML. The orange boxes indicate the effort required by expert annotators. Instead of having to label extensive quantities of data by hand, the effort in data programming framework lies in obtaining labeling functions. In KeyClass, these labeling functions are our keyword-matching rules automatically extracted from reference data, to further reduce required human effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='classdesc'></a>\n",
    "### 2.3 Find Class Descriptions\n",
    "Unlike traditional supervised learning where each document needs to be labeled, KeyClass only relies on meaningful and succinct class descriptions, also removing the requirement of expert heuristics as in prior weak supervision work. As a concrete example, let's consider the IMDb movie review sentiment classification problem, where the objective is to classify a movie re-\n",
    "view as being `positive` or `negative`. In order to initiate the classification process, domain experts provide <code><b>KeyClass</b></code> with common sense descriptions of a __positive__ (`good amazing exciting positive`) and __negative review__ (`terrible bad boring negative`). In most cases, these descriptions can be automatically generated from Wikipedia articles or reference manuals and validated by domain experts, further reducing manual effort. Class Descriptions used in this tutorial can be found [here](./config_files/config_imdb.yml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='keywords'></a>\n",
    "### 2.4 Find Relevant Keywords / Encoding the Dataset\n",
    "\n",
    "Once we have the class descriptions, KeyClass automatically discovers highly suggestive keywords and phrases for each class. KeyClass first obtains frequent n-grams from the training corpus to serve as keywords or key-phrases for its automatically composed labeling functions. In order to transform the keywords into labeling functions of the prescribed form, KeyClass leverage the general linguistic knowledge stored within pre-trained neural language models such as Bidirectional Encoder Representations from Transformers __(BERT)__<sup><a href=\"#references\"><b>14</b></a></sup>, to map each keyword to the most semantically related category description. To create a labeling function, KeyClass simply assigns a keyword to its closest category as measured by the cosine similarity between their embeddings. In order to ensure equal representation of all classes, KeyClass sub-samples the top-k labeling functions per class, ordering them by cosine similarity. While theoretically data programming benefits from as many labeling functions as possible, the sampling is required due to computational and space constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import sys\n",
    "sys.path.append('../keyclass/')\n",
    "sys.path.append('../scripts/')\n",
    "\n",
    "import argparse\n",
    "import label_data, encode_datasets, train_downstream_model\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join, exists\n",
    "from datetime import datetime\n",
    "import utils\n",
    "import models\n",
    "import create_lfs\n",
    "import train_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input arguments\n",
    "config_file_path = r'../config_files/config_imdb.yml' # Specify path to the configuration file\n",
    "random_seed = 0 # Random seed for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: paraphrase-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6eaf2f871c414b8b2ecf819e2d9356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74d9bae3e13418fb2bea06716fe8751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = utils.Parser(config_file_path=config_file_path).parse()\n",
    "\n",
    "if args['use_custom_encoder']:\n",
    "    model = models.CustomEncoder(pretrained_model_name_or_path=args['base_encoder'], \n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    model = models.Encoder(model_name=args['base_encoder'], \n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    sentences = utils.fetch_data(dataset=args['dataset'], split=split, path=args['data_path'])\n",
    "    embeddings = model.encode(sentences=sentences, batch_size=args['end_model_batch_size'], \n",
    "                                show_progress_bar=args['show_progress_bar'], \n",
    "                                normalize_embeddings=args['normalize_embeddings'])\n",
    "    with open(join(args['data_path'], args['dataset'], f'{split}_embeddings.pkl'), 'wb') as f:\n",
    "        pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='label'></a>\n",
    "### 2.5 Probabilistically Labeling the Data\n",
    "\n",
    "Next, _KeyClass_ constructs the labeling function vote matrix and generates probabilistic labels for all training documents using a label model. Specifically, we use the open-source label model implementation of the ___Snorkel Python library___<sup><a href=\"#references\"><b>13</b></a></sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: paraphrase-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting labels for the imdb data...\n",
      "Size of the data: 25000\n",
      "Class distribution (array([0, 1]), array([12500, 12500]))\n",
      "Found assigned category counts [6789 9578]\n",
      "labeler.vocabulary:\n",
      " 16367\n",
      "labeler.word_indicator_matrix.shape (25000, 600)\n",
      "Len keywords 600\n",
      "assigned_category: Unique and Counts (array([0, 1]), array([300, 300]))\n",
      "negative, hate, expensive, bad, poor, broke, waste, horrible, would not recommend ['abominable' 'abomination' 'absolute worst' 'absolutely awful'\n",
      " 'absolutely terrible' 'abuse' 'abused' 'abusive' 'abysmal'\n",
      " 'acting horrible' 'acting poor' 'acting terrible' 'actors bad'\n",
      " 'actually bad' 'also bad' 'among worst' 'annoyance' 'annoying' 'appalled'\n",
      " 'appalling' 'atrocious' 'awful' 'awfully' 'awfulness' 'bad' 'bad actor'\n",
      " 'bad actors' 'bad actually' 'bad almost' 'bad bad' 'bad could'\n",
      " 'bad either' 'bad enough' 'bad even' 'bad film' 'bad films' 'bad get'\n",
      " 'bad horror' 'bad idea' 'bad like' 'bad made' 'bad makes' 'bad many'\n",
      " 'bad movie' 'bad movies' 'bad music' 'bad one' 'bad ones' 'bad people'\n",
      " 'bad really' 'bad reviews' 'bad special' 'bad story' 'bad taste'\n",
      " 'bad thing' 'bad things' 'bad think' 'bad way' 'bad well' 'bad would'\n",
      " 'baddies' 'badly' 'badly made' 'badness' 'best worst' 'better worse'\n",
      " 'big disappointment' 'chose' 'complain' 'complained' 'complaining'\n",
      " 'complains' 'crap like' 'crappy' 'criticism' 'criticisms' 'criticize'\n",
      " 'criticized' 'cursed' 'cursing' 'cynical' 'cynicism' 'depressed'\n",
      " 'depressing' 'despair' 'desperation' 'despicable' 'despise'\n",
      " 'disappointed' 'disappointing' 'disappointment' 'disastrous' 'disdain'\n",
      " 'disgust' 'disgusted' 'disgusting' 'dislike' 'disliked' 'dismal'\n",
      " 'displeasure' 'distasteful' 'distraught' 'dreaded' 'dreadful'\n",
      " 'dreadfully' 'easily worst' 'else say' 'even bad' 'even worst'\n",
      " 'far worse' 'far worst' 'feel bad' 'feel sorry' 'film awful' 'film bad'\n",
      " 'film terrible' 'film worst' 'films bad' 'filth' 'filthy' 'get bad'\n",
      " 'god awful' 'greed' 'hapless' 'hate' 'hate film' 'hate movie' 'hated'\n",
      " 'hated movie' 'hateful' 'hates' 'hating' 'hatred' 'hideous' 'hideously'\n",
      " 'honestly say' 'horrendous' 'horrible' 'horrible film' 'horrible movie'\n",
      " 'horribly' 'horrid' 'horrific' 'huge disappointment' 'humble opinion'\n",
      " 'idiotic' 'immoral' 'incredibly bad' 'incredibly stupid' 'irresponsible'\n",
      " 'know bad' 'lack' 'lackluster' 'laughably bad' 'least good' 'like bad'\n",
      " 'like cheap' 'like horror' 'like least' 'like say' 'loathing' 'lot bad'\n",
      " 'lousy' 'love bad' 'love hate' 'low quality' 'made bad'\n",
      " 'major disappointment' 'make bad' 'many bad' 'mean spirited' 'mediocrity'\n",
      " 'miserable' 'miserably' 'misery' 'misfortune' 'misguided' 'movie awful'\n",
      " 'movie bad' 'movie horrible' 'movie terrible' 'movie worst' 'movies bad'\n",
      " 'much worse' 'nasty' 'needless say' 'needlessly' 'negative comments'\n",
      " 'negative reviews' 'never want' 'nothing good' 'notorious' 'one awful'\n",
      " 'one bad' 'one worst' 'opinion' 'opinions' 'pathetic' 'people hate'\n",
      " 'pity' 'plain awful' 'plain bad' 'plain stupid' 'poor' 'poor quality'\n",
      " 'poorest' 'poorly' 'possibly worst' 'poverty' 'pretty awful' 'pretty bad'\n",
      " 'pretty lame' 'pretty poor' 'probably worst' 'quite bad' 'rather poor'\n",
      " 'really annoying' 'really awful' 'really bad' 'really disappointed'\n",
      " 'really hate' 'really poor' 'really sad' 'really stupid'\n",
      " 'really terrible' 'refuse' 'regret' 'regrets' 'reject' 'repulsive'\n",
      " 'rotten' 'sad' 'sad thing' 'saddest' 'sadistic' 'sadly' 'sadness'\n",
      " 'say bad' 'say worst' 'see bad' 'seedy' 'simply awful' 'something bad'\n",
      " 'still bad' 'story bad' 'stupid' 'stupidest' 'stupidity' 'stupidly'\n",
      " 'sucks' 'terrible' 'terrible film' 'terrible movie' 'terribly'\n",
      " 'think bad' 'tiresome' 'trash' 'trashy' 'truly awful' 'truly bad' 'ugly'\n",
      " 'unappealing' 'unattractive' 'unbearably' 'uneducated' 'unfortunate'\n",
      " 'unfortunately' 'unfortunately film' 'unhappy' 'unimaginative'\n",
      " 'unimpressive' 'uninteresting' 'unlikable' 'unlikeable' 'unlucky'\n",
      " 'unnecessarily' 'unpleasant' 'unrealistic' 'unremarkable' 'unsatisfied'\n",
      " 'unsatisfying' 'unsympathetic' 'unwilling' 'waste money' 'worse'\n",
      " 'worse movie' 'worse movies' 'worst' 'worst acting' 'worst ever'\n",
      " 'worst film' 'worst films' 'worst kind' 'worst movie' 'worst movies'\n",
      " 'worst part' 'worst thing' 'worthless' 'wretched' 'writing bad']\n",
      "good, positive, excellent, amazing, love, fine, good quality, would recommend ['actually good' 'actually like' 'admirable' 'admirably' 'admired'\n",
      " 'almost good' 'also enjoyed' 'also excellent' 'also good' 'also great'\n",
      " 'also interesting' 'also like' 'also liked' 'also nice' 'also pretty'\n",
      " 'also well' 'always good' 'always great' 'among best' 'another good'\n",
      " 'another great' 'another reviewer' 'anyone likes' 'anything good'\n",
      " 'award best' 'bad good' 'best' 'best best' 'best ever' 'best one'\n",
      " 'best parts' 'best performance' 'best show' 'best thing' 'best things'\n",
      " 'better ones' 'certainly good' 'commendable' 'damn good'\n",
      " 'definitely good' 'definitely recommend' 'definitely worth' 'done good'\n",
      " 'done great' 'enjoy' 'enjoy good' 'enjoyable' 'enjoyable film'\n",
      " 'enjoyable movie' 'enjoyable watch' 'enough good' 'entertainment value'\n",
      " 'especially good' 'especially like' 'especially liked' 'even good'\n",
      " 'even great' 'excellence' 'excellent' 'excellent film' 'excellent job'\n",
      " 'excellent movie' 'excellent performance' 'excellent performances'\n",
      " 'excellently' 'exquisite' 'extremely well' 'fabulous' 'fairly good'\n",
      " 'fantastic' 'far best' 'film excellent' 'find good' 'fine job'\n",
      " 'fine performances' 'finest' 'first rate' 'get good' 'give good'\n",
      " 'gives best' 'gives good' 'gives great' 'good' 'good action' 'good also'\n",
      " 'good although' 'good bad' 'good choice' 'good direction' 'good either'\n",
      " 'good enough' 'good entertainment' 'good especially' 'good even'\n",
      " 'good example' 'good film' 'good films' 'good first' 'good good'\n",
      " 'good great' 'good idea' 'good movie' 'good music' 'good one' 'good ones'\n",
      " 'good original' 'good part' 'good parts' 'good people' 'good performance'\n",
      " 'good performances' 'good really' 'good reviews' 'good say' 'good show'\n",
      " 'good special' 'good stuff' 'good taste' 'good thing' 'good things'\n",
      " 'good think' 'good though' 'good tv' 'good use' 'good well' 'good work'\n",
      " 'good would' 'good writing' 'got good' 'got great' 'great'\n",
      " 'great character' 'great example' 'great film' 'great fun' 'great love'\n",
      " 'great music' 'great one' 'great really' 'great show' 'great supporting'\n",
      " 'great things' 'great time' 'greats' 'high quality' 'high rating'\n",
      " 'highly recommend' 'highly recommended' 'however like' 'idea good'\n",
      " 'like best' 'like good' 'like great' 'liked' 'liked one' 'looks great'\n",
      " 'lot good' 'lot great' 'love good' 'lovely' 'luxury' 'made good'\n",
      " 'made great' 'made well' 'make good' 'make great' 'makes good'\n",
      " 'makes great' 'many good' 'many great' 'many reviewers' 'many reviews'\n",
      " 'marvelously' 'may good' 'mean good' 'might good' 'movie excellent'\n",
      " 'movie good' 'movie recommend' 'movie wonderful' 'much enjoyed'\n",
      " 'much good' 'music good' 'music great' 'nearly good' 'nice' 'nice look'\n",
      " 'nothing better' 'one best' 'one finest' 'one good' 'one great'\n",
      " 'one like' 'overall good' 'overall think' 'particularly good'\n",
      " 'people good' 'people like' 'performances good' 'perhaps best'\n",
      " 'personal favorite' 'personally think' 'pleasant' 'positive reviews'\n",
      " 'positive thing' 'possibly best' 'praise' 'prefer' 'prefers'\n",
      " 'pretty decent' 'pretty good' 'probably best' 'probably good'\n",
      " 'probably like' 'put good' 'qualities' 'quality' 'quality acting'\n",
      " 'quite enjoyable' 'quite good' 'quite like' 'rather good' 'rating 10'\n",
      " 'read review' 'read reviews' 'reading reviews' 'real good'\n",
      " 'really appreciate' 'really enjoy' 'really enjoyed' 'really good'\n",
      " 'really great' 'really like' 'really liked' 'really loved' 'really nice'\n",
      " 'really recommend' 'recommend' 'recommend anyone' 'recommend everyone'\n",
      " 'recommend film' 'recommend movie' 'recommend one' 'recommend see'\n",
      " 'recommend watch' 'recommend watching' 'recommendation' 'recommended'\n",
      " 'recommending' 'redeeming quality' 'reviews' 'satisfactory' 'say best'\n",
      " 'say good' 'see good' 'seen good' 'show good' 'solid performances'\n",
      " 'something better' 'something good' 'something interesting' 'splendid'\n",
      " 'still enjoyable' 'still good' 'still great' 'strongly recommend'\n",
      " 'surprisingly good' 'tasteful' 'terrific' 'thing good' 'think best'\n",
      " 'think good' 'think great' 'though good' 'thought good' 'thought great'\n",
      " 'time great' 'top notch' 'truly great' 'two best' 'want good'\n",
      " 'watch good' 'well crafted' 'well good' 'well great' 'well made'\n",
      " 'well produced' 'well worth' 'wonderful' 'wonderful film' 'wonderful job'\n",
      " 'wonderful life' 'wonderful movie' 'wonderfully' 'worth look'\n",
      " 'worth mentioning' 'worth seeing' 'worthwhile' 'would good'\n",
      " 'would recommend']\n",
      "==== Training the label model ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n",
      "INFO:root:Using GPU...\n",
      "  0%|          | 0/100 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=0.191]\n",
      "100%|██████████| 100/100 [00:05<00:00, 16.82epoch/s]\n",
      "INFO:root:Finished Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Model Predictions: Unique value and counts (array([0, 1]), array([ 8914, 16086]))\n",
      "Label Model Training Accuracy 0.70016\n",
      "Saving results in ../results/imdb/train_label_model_with_ground_truth_07-Sep-2022-23_41_16.txt...\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='train')\n",
    "\n",
    "training_labels_present = False\n",
    "if exists(join(args['data_path'], args['dataset'], 'train_labels.txt')):\n",
    "    with open(join(args['data_path'], args['dataset'], 'train_labels.txt'), 'r') as f:\n",
    "        y_train = f.readlines()\n",
    "    y_train = np.array([int(i.replace('\\n','')) for i in y_train])\n",
    "    training_labels_present = True\n",
    "else:\n",
    "    y_train = None\n",
    "    training_labels_present = False\n",
    "    print('No training labels found!')\n",
    "\n",
    "with open(join(args['data_path'], args['dataset'], 'train_embeddings.pkl'), 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"Getting labels for the {args['dataset']} data...\")\n",
    "print(f'Size of the data: {len(train_text)}')\n",
    "if training_labels_present:\n",
    "    print('Class distribution', np.unique(y_train, return_counts=True))\n",
    "\n",
    "# Load label names/descriptions\n",
    "label_names = []\n",
    "for a in args:\n",
    "    if 'target' in a: label_names.append(args[a])\n",
    "\n",
    "# Creating labeling functions\n",
    "labeler = create_lfs.CreateLabellingFunctions(base_encoder=args['base_encoder'], \n",
    "                                            device=torch.device(args['device']),\n",
    "                                            label_model=args['label_model'])\n",
    "proba_preds = labeler.get_labels(text_corpus=train_text, label_names=label_names, min_df=args['min_df'], \n",
    "                                ngram_range=args['ngram_range'], topk=args['topk'], y_train=y_train, \n",
    "                                label_model_lr=args['label_model_lr'], label_model_n_epochs=args['label_model_n_epochs'], \n",
    "                                verbose=True, n_classes=args['n_classes'])\n",
    "\n",
    "y_train_pred = np.argmax(proba_preds, axis=1)\n",
    "\n",
    "# Save the predictions\n",
    "if not os.path.exists(args['preds_path']): os.makedirs(args['preds_path'])\n",
    "with open(join(args['preds_path'], f\"{args['label_model']}_proba_preds.pkl\"), 'wb') as f:\n",
    "    pickle.dump(proba_preds, f)\n",
    "\n",
    "# Print statistics\n",
    "print('Label Model Predictions: Unique value and counts', np.unique(y_train_pred, return_counts=True))\n",
    "if training_labels_present:\n",
    "    print('Label Model Training Accuracy', np.mean(y_train_pred==y_train))\n",
    "\n",
    "    # Log the metrics\n",
    "    training_metrics_with_gt = utils.compute_metrics(y_preds=y_train_pred, y_true=y_train, average=args['average'])\n",
    "    utils.log(metrics=training_metrics_with_gt, filename='label_model_with_ground_truth', \n",
    "        results_dir=args['results_path'], split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exp_training'></a>\n",
    "## 3. Experimentation: Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='downstream'></a>\n",
    "### 3.1 Training the Downstream Model\n",
    "\n",
    "After obtaining a probabilistically labeled training dataset, KeyClass can train any downstream classifier using rich document feature representations provided by the neural language model. Instead of using all the automatically labeled documents, KeyClass initially trains the downstream classifier using top-$k$ documents with the most confident label estimates only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: paraphrase-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence of least confident data point of class 0: 0.9118951704039087\n",
      "Confidence of least confident data point of class 1: 0.9999157388534687\n",
      "\n",
      "==== Data statistics ====\n",
      "Size of training data: (25000, 768), testing data: (25000, 768)\n",
      "Size of testing labels: (25000,)\n",
      "Size of training labels: (25000,)\n",
      "Training class distribution (ground truth): [0.5 0.5]\n",
      "Training class distribution (label model predictions): [0.35656 0.64344]\n",
      "\n",
      "KeyClass only trains on the most confidently labeled data points! Applying mask...\n",
      "\n",
      "==== Data statistics (after applying mask) ====\n",
      "Size of training data: (7000, 768)\n",
      "Size of training labels: (7000,)\n",
      "Training class distribution (ground truth): [0.55057143 0.44942857]\n",
      "Training class distribution (label model predictions): [0.5 0.5]\n",
      "\n",
      "===== Training the downstream classifier =====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18:  90%|█████████ | 18/20 [00:03<00:00,  5.99batch/s, best_loss=0.561, running_loss=0.562, tolerance_count=3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early...\n"
     ]
    }
   ],
   "source": [
    "args = utils.Parser(config_file_path=config_file_path).parse()\n",
    "\n",
    "# Set random seeds\n",
    "random_seed = random_seed\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "X_train_embed_masked, y_train_lm_masked, y_train_masked, \\\n",
    "\tX_test_embed, y_test, training_labels_present, \\\n",
    "\tsample_weights_masked, proba_preds_masked = train_downstream_model.load_data(args)\n",
    "\n",
    "# Train a downstream classifier\n",
    "\n",
    "if args['use_custom_encoder']:\n",
    "\tencoder = models.CustomEncoder(pretrained_model_name_or_path=args['base_encoder'], device=args['device'])\n",
    "else:\n",
    "\tencoder = models.Encoder(model_name=args['base_encoder'], device=args['device'])\n",
    "\n",
    "classifier = models.FeedForwardFlexible(encoder_model=encoder,\n",
    "\t\t\t\t\t\t\t\t\t\th_sizes=args['h_sizes'], \n",
    "\t\t\t\t\t\t\t\t\t\tactivation=eval(args['activation']),\n",
    "\t\t\t\t\t\t\t\t\t\tdevice=torch.device(args['device']))\n",
    "print('\\n===== Training the downstream classifier =====\\n')\n",
    "model = train_classifier.train(model=classifier, \n",
    "\t\t\t\t\t\t\tdevice=torch.device(args['device']),\n",
    "\t\t\t\t\t\t\tX_train=X_train_embed_masked, \n",
    "\t\t\t\t\t\t\ty_train=y_train_lm_masked,\n",
    "\t\t\t\t\t\t\tsample_weights=sample_weights_masked if args['use_noise_aware_loss'] else None, \n",
    "\t\t\t\t\t\t\tepochs=args['end_model_epochs'], \n",
    "\t\t\t\t\t\t\tbatch_size=args['end_model_batch_size'], \n",
    "\t\t\t\t\t\t\tcriterion=eval(args['criterion']), \n",
    "\t\t\t\t\t\t\traw_text=False, \n",
    "\t\t\t\t\t\t\tlr=eval(args['end_model_lr']), \n",
    "\t\t\t\t\t\t\tweight_decay=eval(args['end_model_weight_decay']),\n",
    "\t\t\t\t\t\t\tpatience=args['end_model_patience'])\n",
    "\n",
    "\n",
    "end_model_preds_train = model.predict_proba(torch.from_numpy(X_train_embed_masked), batch_size=512, raw_text=False)\n",
    "end_model_preds_test = model.predict_proba(torch.from_numpy(X_test_embed), batch_size=512, raw_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='self'></a>\n",
    "### 3.2 Self-Training the Model\n",
    "Finally, KeyClass self-trains the downstream model-encoder combination on the entire training dataset to refine the end model classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15:  24%|██▍       | 15/62 [27:40<1:26:42, 110.69s/batch, self_train_agreement=1, tolerance_count=2, validation_accuracy=0.864]    \n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[[0.8638912  0.0019224 ]\n",
      " [0.86391417 0.00192269]\n",
      " [0.8638912  0.0019224 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    2.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Fetching the raw text data for self-training\n",
    "X_train_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='train')\n",
    "X_test_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='test')\n",
    "\n",
    "model = train_classifier.self_train(model=model, \n",
    "\t\t\t\t\t\t\t\t\tX_train=X_train_text, \n",
    "\t\t\t\t\t\t\t\t\tX_val=X_test_text, \n",
    "\t\t\t\t\t\t\t\t\ty_val=y_test, \n",
    "\t\t\t\t\t\t\t\t\tdevice=torch.device(args['device']), \n",
    "\t\t\t\t\t\t\t\t\tlr=eval(args['self_train_lr']), \n",
    "\t\t\t\t\t\t\t\t\tweight_decay=eval(args['self_train_weight_decay']),\n",
    "\t\t\t\t\t\t\t\t\tpatience=args['self_train_patience'], \n",
    "\t\t\t\t\t\t\t\t\tbatch_size=args['self_train_batch_size'], \n",
    "\t\t\t\t\t\t\t\t\tq_update_interval=args['q_update_interval'],\n",
    "\t\t\t\t\t\t\t\t\tself_train_thresh=eval(args['self_train_thresh']), \n",
    "\t\t\t\t\t\t\t\t\tprint_eval=True)\n",
    "\n",
    "\n",
    "end_model_preds_test = model.predict_proba(X_test_text, batch_size=args['self_train_batch_size'], raw_text=True)\n",
    "\n",
    "\n",
    "# Print statistics\n",
    "testing_metrics = utils.compute_metrics_bootstrap(y_preds=np.argmax(end_model_preds_test, axis=1),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_test, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tn_bootstrap=args['n_bootstrap'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tn_jobs=args['n_jobs'])\n",
    "print(testing_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exp_testing'></a>\n",
    "## 4. Experimentation: Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence of least confident data point of class 0: 0.9118951704039087\n",
      "Confidence of least confident data point of class 1: 0.9999157388534687\n",
      "\n",
      "==== Data statistics ====\n",
      "Size of training data: (25000, 768), testing data: (25000, 768)\n",
      "Size of testing labels: (25000,)\n",
      "Size of training labels: (25000,)\n",
      "Training class distribution (ground truth): [0.5 0.5]\n",
      "Training class distribution (label model predictions): [0.35656 0.64344]\n",
      "\n",
      "KeyClass only trains on the most confidently labeled data points! Applying mask...\n",
      "\n",
      "==== Data statistics (after applying mask) ====\n",
      "Size of training data: (7000, 768)\n",
      "Size of training labels: (7000,)\n",
      "Training class distribution (ground truth): [0.55057143 0.44942857]\n",
      "Training class distribution (label model predictions): [0.5 0.5]\n",
      "training_metrics_with_gt [0.8847142857142857, 0.8908748478426236, 0.8847142857142857]\n",
      "training_metrics_with_lm [0.8935714285714286, 0.893620301753385, 0.8935714285714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing_metrics [[0.8160768  0.00244514]\n",
      " [0.82950237 0.0021809 ]\n",
      " [0.8160768  0.00244514]]\n",
      "\n",
      "===== Self-training the downstream classifier =====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  40 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing_metrics after self train [[0.8714468  0.00204239]\n",
      " [0.87203099 0.0020189 ]\n",
      " [0.8714468  0.00204239]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "end_model_path='../models/imdb/end_model_26-Jul-2022-03_29_41.pth'\n",
    "end_model_self_trained_path='../models/imdb/end_model_self_trained_26 Jul 2022 03:59:43.pth'\n",
    "\n",
    "args = utils.Parser(config_file_path=config_file_path).parse()\n",
    "\n",
    "# Set random seeds\n",
    "random_seed = random_seed\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "X_train_embed_masked, y_train_lm_masked, y_train_masked, \\\n",
    "\tX_test_embed, y_test, training_labels_present, \\\n",
    "\tsample_weights_masked, proba_preds_masked = train_downstream_model.load_data(args)\n",
    "\n",
    "model = torch.load(end_model_path)\n",
    "\n",
    "end_model_preds_train = model.predict_proba(torch.from_numpy(X_train_embed_masked), batch_size=512, raw_text=False)\n",
    "end_model_preds_test = model.predict_proba(torch.from_numpy(X_test_embed), batch_size=512, raw_text=False)\n",
    "\n",
    "# Print statistics\n",
    "if training_labels_present:\n",
    "\ttraining_metrics_with_gt = utils.compute_metrics(y_preds=np.argmax(end_model_preds_train, axis=1), \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_train_masked, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'])\n",
    "\tprint('training_metrics_with_gt', training_metrics_with_gt)\n",
    "\n",
    "training_metrics_with_lm = utils.compute_metrics(y_preds=np.argmax(end_model_preds_train, axis=1), \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_train_lm_masked, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'])\n",
    "print('training_metrics_with_lm', training_metrics_with_lm)\n",
    "\n",
    "testing_metrics = utils.compute_metrics_bootstrap(y_preds=np.argmax(end_model_preds_test, axis=1), \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_test, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tn_bootstrap=args['n_bootstrap'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tn_jobs=args['n_jobs'])\n",
    "print('testing_metrics', testing_metrics)\n",
    "\n",
    "\n",
    "print('\\n===== Self-training the downstream classifier =====\\n')\n",
    "\n",
    "# Fetching the raw text data for self-training\n",
    "X_train_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='train')\n",
    "X_test_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='test')\n",
    "\n",
    "model = torch.load(end_model_self_trained_path)\n",
    "\n",
    "end_model_preds_test = model.predict_proba(X_test_text, batch_size=args['self_train_batch_size'], raw_text=True)\n",
    "\n",
    "\n",
    "# Print statistics\n",
    "testing_metrics = utils.compute_metrics_bootstrap(y_preds=np.argmax(end_model_preds_test, axis=1),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_test, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tn_bootstrap=args['n_bootstrap'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tn_jobs=args['n_jobs'])\n",
    "print('testing_metrics after self train', testing_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "## 5. References \n",
    "\n",
    "[[1](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3270933/)] Nir Menachemi and Taleah H Collum. Benefits and drawbacks of electronic health record systems. Risk management and healthcare policy, 4:47, 2011.\n",
    "\n",
    "[[2](https://academic.oup.com/jamia/article/24/6/1142/4091350)] Julia Adler-Milstein, A Jay Holmgren, Peter Kralovec, Chantal Worzala, Talisha Searcy, and Vaishali Patel. Electronic health record adoption in us hospitals: the emergence of a digital “advanced use” divide. Journal of the American Medical Informatics Association, 24(6):1142–1148, 2017.\n",
    "\n",
    "[[3](https://www.tandfonline.com/doi/full/10.1080/2331205X.2021.1893422)] Musaed Ali Alharbi, Godfrey Isouard, and Barry Tolchard. Historical development of the statistical classification of causes of death and diseases. Cogent Medicine, 8(1):1893422, 2021. doi: 10.1080/2331205X.2021.1893422. URL https://doi.org/10.1080/2331205X.2021.1893422.\n",
    "\n",
    "[[4](https://n.neurology.org/content/49/3/660.short)] Curtis Benesch, DM Witter, AL Wilder, PW Duncan, GP Samsa, and DB Matchar. Inaccuracy of the international classification of diseases (icd-9-cm) in identifying the diagnosis of ischemic cerebrovascular disease. Neurology, 49(3):660–664, 1997.\n",
    "\n",
    "[[5](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0234647)] Guhan Ram Venkataraman, Arturo Lopez Pineda, Oliver J Bear Don’t Walk IV, Ashley M Zehnder, Sandeep Ayyar, Rodney L Page, Carlos D Bustamante, and Manuel A Rivas. Fastag: Automatic text classification of unstructured medical narratives. PLoS one, 15(6):e0234647, 2020.\n",
    "\n",
    "[[6](https://www.aaai.org/ocs/index.php/WS/AAAIW18/paper/view/16881/0)] Tal Baumel, Jumana Nassour-Kassis, Raphael Cohen, Michael Elhadad, and No ́emie El- hadad. Multi-label classification of patient notes: case study on icd code assignment. In Workshops at the thirty-second AAAI conference on artificial intelligence, 2018.\n",
    "\n",
    "[[7]()] Sepp Hochreiter and J ̈urgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735–1780, 11 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735.\n",
    "\n",
    "[[8](https://link.springer.com/chapter/10.1007/978-3-319-21843-4_12)] Stefano Giovanni Rizzo, Danilo Montesi, Andrea Fabbri, and Giulio Marchesini. Icd code retrieval: Novel approach for assisted disease classification. In International Conference on Data Integration in the Life Sciences, pages 147–161. Springer, 2015.\n",
    "\n",
    "[[9](https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-259.pdf?ref=https://githubhelp.com)] Evgeniy Gabrilovich, Shaul Markovitch, et al. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In IJcAI, volume 7, pages 1606–1611, 2007.\n",
    "\n",
    "[[10](https://arxiv.org/abs/2010.07245)] Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong, Heng Ji, Chao Zhang, and Jiawei Han. Text classification using label names only: A language model self-training approach. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro- cessing, 2020.\n",
    "\n",
    "[[11](https://link.springer.com/article/10.1186/s12911-018-0723-6)] Yanshan Wang, Sunghwan Sohn, Sijia Liu, Feichen Shen, Liwei Wang, Elizabeth J Atkinson, Shreyasee Amin, and Hongfang Liu. A clinical text classification paradigm using weak supervision and deep representation. BMC medical informatics and decision making, 19(1):1–13, 2019.\n",
    "\n",
    "[[12](https://www.sciencedirect.com/science/article/pii/S0022395621000637)] Marika Cusick, Prakash Adekkanattu, Thomas R Campion Jr, Evan T Sholle, Annie Myers, Samprit Banerjee, George Alexopoulos, Yanshan Wang, and Jyotishman Pathak. Using weak supervision and deep learning to classify clinical notes for identification of current suicidal ideation. Journal of psychiatric research, 136:95–102, 2021.\n",
    "\n",
    "[[13](https://proceedings.neurips.cc/paper/2016/hash/6709e8d64a5f47269ed5cea9f625f7ab-Abstract.html)] Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R ́e. Data programming: Creating large training sets, quickly. In Advances in neural infor- mation processing systems, pages 3567–3575, 2016.\n",
    "\n",
    "[[14](https://arxiv.org/abs/1810.04805)] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171– 4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('keyclass')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "318bfa02e7908bdce70328a8696b89c0eda4a9244e2fc5762a68deeb8a3ec1f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
